{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "六轴动作数据通过小程序game模式在50Hz频率下采样，分别为开合跳、蹲起、合掌跳、高抬腿，同时存储记录人身高、体重、年龄和性别信息。\n",
    "通过LSTM、CNN和MLP算法均可以实现较好动作检测，其中测试集准确率LSTM=89.6% > MLP=86% >= CNN=86.24%。\n",
    "由于个人采集数据量较小且动作之间数据量不平衡，实时监测准确率稍有下降，但扩充数据集重新导入参数即可大幅度改良。\n",
    "（三种算法均测试过其他同学多种动作数据集，准确率在87%以上）\n",
    "通过JavaScript重写MLP源码部署在小程序上，可以实现离线识别，但语音播报需要联网接通云存储音频文件。\n",
    "由于小程序内部计算量大，采样频率受到影响，但大体监测仍准确。若改用连接服务器则使用LSTM、CNN算法识别效果会更好。\n",
    "以下分数据预处理和CNN、MLP、LSTM算法两部分介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#数据预处理：小程序云数据库存储的数据导出为csv格式，分别由加速计XYZ轴（accXs,accYs,accZs)、陀螺仪XYZ轴（gyroXs,gyroYs,gyroZs)、\n",
    "#对应采集加速计和陀螺仪信号的时间戳（AtimeSs，GtimeSs）、身高体重年龄及性别、所做运动标签（yundong）和记录人id\n",
    "#数据预处理将原采集的数据转化成方便算法分割读取的形式\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "writer=pd.ExcelWriter('hezhangtiao.xlsx')\n",
    "\n",
    "def read_data(file_path):\n",
    "    data = pd.read_csv(file_path)  #读取csv数据\n",
    "    return data\n",
    "\n",
    "dataset=read_data('hezhangtiao.csv')\n",
    "#\n",
    "for j in range(len(dataset)):   #读取的csv数据是字符串格式，将其分割成数组，转化成浮点数，再写入excel\n",
    "    Atimes_list = dataset['AtimeSs'][j][1:-1].split(',')\n",
    "    Gtimes_list = dataset['GtimeSs'][j][1:-1].split(',')\n",
    "    size=min(len(Atimes_list),len(Gtimes_list))    #加速计和陀螺仪由于采样误差可能数据集长度会相差1个左右，保留二者相同长度数据\n",
    "    Atimes_float = [float(i) for i in Atimes_list][0:size]\n",
    "    Gtimes_float = [float(i) for i in Gtimes_list][0:size]\n",
    "    accXs_list=dataset['accXs'][j][1:-1].split(',')\n",
    "    accXs_float=[float(i) for i in accXs_list][0:size]\n",
    "    accYs_list=dataset['accYs'][j][1:-1].split(',')\n",
    "    accYs_float=[float(i) for i in accYs_list][0:size]\n",
    "    accZs_list = dataset['accZs'][j][1:-1].split(',')\n",
    "    accZs_float = [float(i) for i in accZs_list][0:size]\n",
    "    gyroXs_list=dataset['gyroXs'][j][1:-1].split(',')\n",
    "    gyroXs_float = [float(i) for i in gyroXs_list][0:size]\n",
    "    gyroYs_list = dataset['gyroYs'][j][1:-1].split(',')\n",
    "    gyroYs_float = [float(i) for i in gyroYs_list][0:size]\n",
    "    gyroZs_list = dataset['gyroZs'][j][1:-1].split(',')\n",
    "    gyroZs_float = [float(i) for i in gyroZs_list][0:size]\n",
    "    weight=[dataset['weight'][j] for i in range(size)]\n",
    "    height=[dataset['height'][j] for i in range(size)]\n",
    "    year=[dataset['year'][j] for i in range(size)]\n",
    "    yundong=[dataset['yundong'][j] for i in range(size)]\n",
    "    openid=[dataset['_openid'][j] for i in range(size)]\n",
    "    gender=[dataset['gender'][j] for i in range(size)]\n",
    "    #转化完数据生成dataframe格式然后写入excel，每一个sheet是一个记录人做的整套动作数据\n",
    "    pd1=pd.DataFrame({'accXs':accXs_float,'accYs':accYs_float,'accZs':accZs_float,\n",
    "                      'gyroXs':gyroXs_float,'gyroYs':gyroYs_float,'gyroZs':gyroZs_float,'AtimeSs':Atimes_float,'GtimeSs':Gtimes_float,\n",
    "                      'weight':weight,'height':height,'year':year, 'yundong':yundong,'openid':openid,'gender':gender})\n",
    "    pd1.to_excel(writer,sheet_name=str(j)+'sheet',index=False)\n",
    "    writer.save()\n",
    "#将四个动作数据集均执行一遍此操作转化为指定格式再合并成一个csv数据集，得到训练算法用的hhyHAR_6data.csv\n",
    "#此处由于数据量小且采集人群均为19-20岁女生，除六轴以外的特征不适合用于训练算法。\n",
    "# 如果扩大数据集至多年龄层、性别平均的数据集则可作为额外特征训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#CNN算法\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "np.random.seed(444)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"  #取消使用GPU，采用cpu跑算法\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()  #如果print(tf.__version__)为2.X以上的版本则需要该两行代码，1.X版本不需要\n",
    "\n",
    "def read_data(file_path):\n",
    "    column_names = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ', 'class'] #读取六轴和标签数据\n",
    "    data = pd.read_csv(file_path, header=None, names=column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset, axis=0)  #数据标准化为均值为0方差为1的数据\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu) / sigma\n",
    "\n",
    "def windows(data, size):    #通过滑动窗口采样，50%overlap重叠\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=128):   #滑动窗口采样2.56s=128个数据\n",
    "    segments = np.empty((0, window_size, 6))  #segment将128个六轴数据整合成（1*128*6）的向量然后拼接所有向量成X集\n",
    "    labels = np.empty((0))              #label将128个六轴数据的动作标签转化成128*1的向量然后拼接所有向量\n",
    "    for (start, end) in windows(data['class'], window_size):\n",
    "        x = data[\"accX\"][start:end]\n",
    "        y = data[\"accY\"][start:end]\n",
    "        z = data[\"accZ\"][start:end]\n",
    "        gx = data[\"gyroX\"][start:end]\n",
    "        gy = data[\"gyroY\"][start:end]\n",
    "        gz = data[\"gyroZ\"][start:end]\n",
    "        if (len(dataset['class'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments, np.dstack([x, y, z, gx, gy, gz])])\n",
    "            labels = np.append(labels, stats.mode(data[\"class\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "\n",
    "def weight_variable(shape):    #权重矩阵初始化\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def depthwise_conv2d(x, W):   #卷积\n",
    "    return tf.nn.depthwise_conv2d(x, W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "\n",
    "def apply_depthwise_conv(x, kernel_size, num_channels, depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights), biases))\n",
    "\n",
    "\n",
    "def apply_max_pool(x, kernel_size, stride_size):   #池化\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1],\n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#读取数据并标准化数据集\n",
    "dataset = read_data('hhyHAR_6data.csv')\n",
    "dataset['accX'] = feature_normalize(dataset['accX'])\n",
    "dataset['accY'] = feature_normalize(dataset['accY'])\n",
    "dataset['accZ'] = feature_normalize(dataset['accZ'])\n",
    "dataset['gyroX'] = feature_normalize(dataset['gyroX'])\n",
    "dataset['gyroY'] = feature_normalize(dataset['gyroY'])\n",
    "dataset['gyroZ'] = feature_normalize(dataset['gyroZ'])\n",
    "\n",
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)  #get dummies独热编码标签，4种动作对应[1,0,0,0],[0,1,0,0]等\n",
    "reshaped_segments = segments.reshape(len(segments), 1, 128, 6)\n",
    "\n",
    "#分割数据集\n",
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "train_x = reshaped_segments[train_test_split]\n",
    "train_y = labels[train_test_split]\n",
    "test_x = reshaped_segments[~train_test_split]\n",
    "test_y = labels[~train_test_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#输入参数\n",
    "input_height = 1\n",
    "input_width = 128  #滑动窗口大小\n",
    "num_labels = 4  #动作分类数\n",
    "num_channels = 6  #输入的X维数（六轴数据）\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 10\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#X是输入，Y是输出，c卷积，p池化，flatten然后输出y\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X, kernel_size, num_channels, depth)\n",
    "p = apply_max_pool(c, 20, 2)\n",
    "c = apply_depthwise_conv(p, 6, depth * num_channels, depth // 10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth // 10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1), f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Training Loss:  12.992293  Training Accuracy:  0.46953404\n",
      "Epoch:  1  Training Loss:  13.448204  Training Accuracy:  0.5878136\n",
      "Epoch:  2  Training Loss:  11.726599  Training Accuracy:  0.6702509\n",
      "Epoch:  3  Training Loss:  10.197773  Training Accuracy:  0.7670251\n",
      "Epoch:  4  Training Loss:  9.0141535  Training Accuracy:  0.83870965\n",
      "Epoch:  5  Training Loss:  8.119954  Training Accuracy:  0.8637993\n",
      "Epoch:  6  Training Loss:  7.428698  Training Accuracy:  0.8781362\n",
      "Epoch:  7  Training Loss:  6.8707523  Training Accuracy:  0.8888889\n",
      "Epoch:  8  Training Loss:  6.411648  Training Accuracy:  0.90681005\n",
      "Epoch:  9  Training Loss:  6.031598  Training Accuracy:  0.9175627\n",
      "Testing Accuracy: 0.80733943\n"
     ]
    }
   ],
   "source": [
    "#迭代更新和计算损失函数、准确率\n",
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):\n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print (\"Epoch: \",epoch,\" Training Loss: \",c,\" Training Accuracy: \",session.run(accuracy, feed_dict={X: train_x, Y: train_y}))\n",
    "    print('Testing Accuracy:', session.run(accuracy, feed_dict={X: test_x, Y: test_y}))\n",
    "    # print('y=',session.run(y_,feed_dict={X : test_x[150:155, :, :, :]}), 'Y=', train_y[150:155])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[72  6  7  8]\n",
      " [ 4 81  3  2]\n",
      " [ 8  2 68  4]\n",
      " [ 5  6  3 41]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.77      0.79        93\n",
      "          1       0.85      0.90      0.88        90\n",
      "          2       0.84      0.83      0.83        82\n",
      "          3       0.75      0.75      0.75        55\n",
      "\n",
      "avg / total       0.82      0.82      0.82       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MLP算法\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "np.random.seed(444)\n",
    "def read_data(file_path):\n",
    "    column_names = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ','class']\n",
    "    data = pd.read_csv(file_path, header=None, names=column_names)\n",
    "    return data\n",
    "\n",
    "dataset=read_data('hhyHAR_6data.csv')\n",
    "\n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=45):\n",
    "    segments = np.empty((0, window_size*6))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['class'], window_size):\n",
    "        x = list(data[\"accX\"][start:end])\n",
    "        y = list(data[\"accY\"][start:end])\n",
    "        z = list(data[\"accZ\"][start:end])\n",
    "        gx = list(data[\"gyroX\"][start:end])\n",
    "        gy = list(data[\"gyroY\"][start:end])\n",
    "        gz = list(data[\"gyroZ\"][start:end])\n",
    "        if (len(dataset['class'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments, x+y+z+gx+gy+gz]) #由于MLP的输入是一维向量，则把滑动窗口取得的六维数据拼接成一维的输入X\n",
    "            labels = np.append(labels, stats.mode(data[\"class\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.array(labels,dtype=int)\n",
    "reshaped_segments=segments.reshape(len(segments),45*6)\n",
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "X_train = reshaped_segments[train_test_split]\n",
    "y_train = labels[train_test_split]\n",
    "X_test = reshaped_segments[~train_test_split]\n",
    "y_test = labels[~train_test_split]\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "#指定激活函数为relu函数  隐藏层有两层，一层130个神经元，一层5个神经元\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(130, 5), max_iter=1000, activation='relu')\n",
    "mlp.fit(X_train, y_train)\n",
    "predictions = mlp.predict(X_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "# print(mlp.classes_)  #可以看各种参数\n",
    "# print(mlp.n_outputs_)\n",
    "# print(mlp.out_activation_)\n",
    "# print(np.array(mlp.coefs_).shape)\n",
    "# print(len(mlp.coefs_))\n",
    "# print(len(mlp.coefs_[0]))\n",
    "# print(len(mlp.intercepts_[0]))\n",
    "# print(np.array(mlp.coefs_[0]))\n",
    "# print(np.array(mlp.coefs_[0]).shape)\n",
    "# print(np.array(mlp.coefs_).shape)\n",
    "# print(np.array(mlp.coefs_[1]).shape)\n",
    "# print(np.array(mlp.coefs_[2]).shape)\n",
    "# print(np.array(mlp.intercepts_).shape)\n",
    "# print(np.array(mlp.intercepts_[0]).shape)\n",
    "# print(np.array(mlp.intercepts_[1]).shape)\n",
    "# print(np.array(mlp.intercepts_[2]).shape)\n",
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#将训练得到的MLP参数导出在excel中，之后再导入小程序js页面data里，就可以通过矩阵运算得到分类值\n",
    "#W是三层的权重矩阵，b是三层的偏置，fX=WX+b,中间激活函数用relu，输出y的分类激活函数用softmax\n",
    "df1=pd.DataFrame(np.array(mlp.coefs_[0]))\n",
    "writer = pd.ExcelWriter('W.xlsx')  \n",
    "df1.to_excel(writer,'w1',index=False)\n",
    "df2=pd.DataFrame(np.array(mlp.coefs_[1]))\n",
    "df2.to_excel(writer,'w2',index=False)\n",
    "df3=pd.DataFrame(np.array(mlp.coefs_[2]))\n",
    "df3.to_excel(writer,'w3',index=False)\n",
    "writer.save()\n",
    "\n",
    "df4=pd.DataFrame(np.array(mlp.intercepts_[0]))\n",
    "writer1 = pd.ExcelWriter('b.xlsx')\n",
    "df4.to_excel(writer1,'b1',index=False)\n",
    "df5=pd.DataFrame(np.array(mlp.intercepts_[1]))\n",
    "df5.to_excel(writer1,'b2',index=False)\n",
    "df6=pd.DataFrame(np.array(mlp.intercepts_[2]))\n",
    "df6.to_excel(writer1,'b3',index=False)\n",
    "writer1.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#LSTM算法\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "np.random.seed(444)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_eager_execution()\n",
    "# tf.disable_v2_behavior()   #tf版本高于2.0才需要这样导入\n",
    "\n",
    "def read_data(file_path):\n",
    "    column_names = ['accX', 'accY', 'accZ', 'gyroX', 'gyroY', 'gyroZ','class']  #class是动作标签\n",
    "    data = pd.read_csv(file_path, header=None, names=column_names)\n",
    "    return data\n",
    "\n",
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu) / sigma\n",
    "\n",
    "\n",
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "\n",
    "def segment_signal(data, window_size=128):\n",
    "    segments = np.empty((0, window_size, 6))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data['class'], window_size):\n",
    "        x = data[\"accX\"][start:end]\n",
    "        y = data[\"accY\"][start:end]\n",
    "        z = data[\"accZ\"][start:end]\n",
    "        gx = data[\"gyroX\"][start:end]\n",
    "        gy = data[\"gyroY\"][start:end]\n",
    "        gz = data[\"gyroZ\"][start:end]\n",
    "        if (len(dataset['class'][start:end]) == window_size):\n",
    "            segments = np.vstack([segments, np.dstack([x, y, z, gx, gy, gz])])\n",
    "            labels = np.append(labels, stats.mode(data[\"class\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "\n",
    "dataset = read_data('hhyHAR_6data.csv')\n",
    "dataset['accX'] = feature_normalize(dataset['accX'])\n",
    "dataset['accY'] = feature_normalize(dataset['accY'])\n",
    "dataset['accZ'] = feature_normalize(dataset['accZ'])\n",
    "dataset['gyroX'] = feature_normalize(dataset['gyroX'])\n",
    "dataset['gyroY'] = feature_normalize(dataset['gyroY'])\n",
    "dataset['gyroZ'] = feature_normalize(dataset['gyroZ'])\n",
    "\n",
    "segments, labels = segment_signal(dataset)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 128, 6)\n",
    "\n",
    "\n",
    "train_test_split = np.random.rand(len(reshaped_segments)) < 0.70\n",
    "X_train = reshaped_segments[train_test_split]\n",
    "y_train = labels[train_test_split]\n",
    "X_test = reshaped_segments[~train_test_split]\n",
    "y_test = labels[~train_test_split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X test shape, y test shape, every Xtest's mean, every Xtest's standard deviation)\n",
      "(109, 128, 6) (109, 4) 0.04011959562112301 1.0255589600638917\n"
     ]
    }
   ],
   "source": [
    "# 输入参数\n",
    "training_data_count = len(X_train)  # 通过滑动窗口取样后的训练集数据个数\n",
    "test_data_count = len(X_test)  # 测试集数据个数\n",
    "n_steps = len(X_train[0])  # 128维的步长（滑动窗口2.56s)\n",
    "n_input = len(X_train[0][0])  # 6维数据\n",
    "#LSTM结构\n",
    "n_hidden = 32 # 隐藏层特征数\n",
    "n_classes = 4 # 4种动作分类\n",
    "#训练超参数\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = training_data_count * 300  #迭代训练300次\n",
    "batch_size = 1500\n",
    "display_iter = 30000  #显示测试集准确率\n",
    "\n",
    "print(\"(X test shape, y test shape, every Xtest's mean, every Xtest's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # 输入(batch_size, n_steps, n_input)\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  \n",
    "    _X = tf.reshape(_X, [-1, n_input])\n",
    "    # reshape后(n_steps*batch_size, n_input)\n",
    "    #激活函数用relu\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    _X = tf.split(_X, n_steps, 0)\n",
    "    \n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    lstm_last_output = outputs[-1]\n",
    "    #输出计算结果\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, step, batch_size):\n",
    "\n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        index = ((step-1)*batch_size + i) % len(_train)\n",
    "        batch_s[i] = _train[index]\n",
    "\n",
    "    return batch_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\envs\\Ma_Learn\\lib\\site-packages\\dask\\dataframe\\utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-09bd507ccdcd>:19: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "#计算损失函数，准确率，迭代更新\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") #防止过拟合\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # softmax损失函数\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost) \n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iter #1500:   Batch Loss = 252.723816, Accuracy = 0.33933332562446594\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 241.0306396484375, Accuracy = 0.4220183491706848\n",
      "Training iter #30000:   Batch Loss = 96.057137, Accuracy = 0.8659999966621399\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 91.11182403564453, Accuracy = 0.7981651425361633\n",
      "Training iter #60000:   Batch Loss = 30.800400, Accuracy = 0.9786666631698608\n",
      "PERFORMANCE ON TEST SET: Batch Loss = 29.50992202758789, Accuracy = 0.8899082541465759\n",
      "训练结束\n",
      "FINAL RESULT: Batch Loss = 12.188701629638672, Accuracy = 0.9082568883895874\n"
     ]
    }
   ],
   "source": [
    "#运行LSTM训练代码\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "step = 1\n",
    "while step * batch_size <= training_iters:\n",
    "    batch_xs = extract_batch_size(X_train, step, batch_size)\n",
    "    batch_ys = extract_batch_size(y_train, step, batch_size)\n",
    "\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs,\n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "\n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "\n",
    "        print(\"Training iter #\" + str(step*batch_size) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy],\n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: y_test\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET: \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"训练结束\")\n",
    "\n",
    "one_hot_predictions, accuracy, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: y_test\n",
    "    }\n",
    ")\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
